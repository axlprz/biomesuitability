# -*- coding: utf-8 -*-
"""DF-Merger_BiomeSuitability.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1InmRVHQosJYRIcDVh_omLNqF2H3zhYzY
"""

# Step 1.1: Install required libraries
!pip install rasterstats geopandas rasterio shapely pyproj openpyxl tqdm

# Step 1.2: Import libraries
import pandas as pd
import geopandas as gpd
import numpy as np
import rasterio
from shapely.geometry import Point, box
import matplotlib.pyplot as plt
from tqdm import tqdm

# Step 1.3: Define target species
target_species = {
    "axolotl": "Ambystoma mexicanum",
    "blue_macaw": "Anodorhynchus hyacinthinus",
    "ocelot": "Leopardus pardalis",
    "black_iguana": "Ctenosaura pectinata",
    "resplendent_quetzal": "Pharomachrus mocinno",
    "coyote": "Canis latrans",
    "crested_caracara": "Caracara cheriway",
    "green_turtle": "Chelonia mydas",
    "american_alligator": "Alligator mississippiensis",
    "asian_elephant": "Elephas maximus"
}

# Step 2: Create land-only grid
def create_grid(bounds, cell_size_deg=1.0):
    minx, miny, maxx, maxy = bounds
    grid_cells = []
    for x0 in np.arange(minx, maxx, cell_size_deg):
        for y0 in np.arange(miny, maxy, cell_size_deg):
            x1 = x0 + cell_size_deg
            y1 = y0 + cell_size_deg
            grid_cells.append(box(x0, y0, x1, y1))
    return gpd.GeoDataFrame({'geometry': grid_cells}, crs="EPSG:4326")

global_bounds = [-180.0, -90.0, 180.0, 90.0]
grid_gdf = create_grid(global_bounds)

# Step 2.1: Filter ocean cells

land_gdf = gpd.read_file("ne_110m_land/ne_110m_land.shp").to_crs(grid_gdf.crs)
grid_gdf = gpd.overlay(grid_gdf, land_gdf, how="intersection")
grid_gdf = grid_gdf[['geometry']]
grid_gdf.sample(1000).plot(edgecolor="gray", facecolor="none", figsize=(10, 6))
plt.title("Sample of Land-Only Grid (1x1 degree)")
plt.show()
print(f"Filtered land-only grid cells: {len(grid_gdf)}")

# Step 3: Load GBIF species occurrence CSVs
species_files = {
    "axolotl": "axolotl.csv",
    "blue_macaw": "bluemacaw.csv",
    "ocelot": "ocelot.csv",
    "black_iguana": "blackiguana.csv",
    "resplendent_quetzal": "quetzal.csv",
    "coyote": "coyote.csv",
    "crested_caracara": "crestedcaracara.csv",
    "green_turtle": "greenturtle.csv",
    "american_alligator": "americanalligator.csv",
    "asian_elephant": "asianelephant.csv"
}
dfs = []
for species, file in species_files.items():
    df = pd.read_csv(file, sep="\t", quotechar='"', low_memory=False, on_bad_lines='skip')
    df = df[['decimalLatitude', 'decimalLongitude', 'eventDate', 'scientificName']].dropna()
    df['species_label'] = species
    dfs.append(df)
gbif_df = pd.concat(dfs, ignore_index=True)
print(f"Combined GBIF dataset shape: {gbif_df.shape}")

# Step 4: Spatial join and species aggregation
geometry = [Point(xy) for xy in zip(gbif_df['decimalLongitude'], gbif_df['decimalLatitude'])]
gbif_gdf = gpd.GeoDataFrame(gbif_df, geometry=geometry, crs="EPSG:4326")
gbif_with_grid = gpd.sjoin(gbif_gdf, grid_gdf, how="inner", predicate="within")
species_counts = gbif_with_grid.groupby(['index_right', 'species_label']).size().unstack(fill_value=0)
grid_with_species = grid_gdf.join(species_counts, how="left").fillna(0)
grid_with_species.head()

# Step 5: Spatial join with WWF Biomes
wwf_gdf = gpd.read_file("wwf/wwf_terr_ecos.shp").to_crs("EPSG:4326")
grid_with_biome = gpd.sjoin(grid_with_species, wwf_gdf[['BIOME', 'geometry']], how="left", predicate="intersects")
grid_with_biome['BIOME'] = grid_with_biome['BIOME'].fillna("Unknown")
grid_with_biome[['BIOME']].value_counts().head()

# Step 6: Extract climatic variables using zonal statistics
import os
from rasterstats import zonal_stats

clim_dir = "worldclim/"
bio_vars = [1, 4, 5, 6, 12, 15, 17, 18]
clim_features = {}
for var in bio_vars:
    tif_path = os.path.join(clim_dir, f"wc2.1_10m_bio_{var}.tif")
    stats = zonal_stats(
        grid_with_biome["geometry"],
        tif_path,
        stats=["mean"],
        nodata=-9999,
        geojson_out=False
    )
    clim_features[f"BIO{var}"] = [s["mean"] if s else None for s in stats]
for col, values in clim_features.items():
    grid_with_biome[col] = values
grid_with_biome.head()

# Step 7: Final cleaning and export
df_cleaned = grid_with_biome.drop(columns=["geometry", "index_right"], errors='ignore')

# Step 7.1: Drop duplicate rows
df_cleaned = df_cleaned.drop_duplicates()

# Step 7.2: Identify species and environmental columns
species_cols = list(target_species.keys())
env_cols = [col for col in df_cleaned.columns if col.startswith("BIO")]

# Step 7.3: Ensure BIO variables are numeric and impute missing values
df_cleaned[env_cols] = df_cleaned[env_cols].apply(pd.to_numeric, errors='coerce')
print("Missing values per column (after coercion):")
print(df_cleaned[env_cols].isna().sum())
df_cleaned[env_cols] = df_cleaned[env_cols].fillna(df_cleaned[env_cols].mean())

# Step 7.4: Convert species columns to binary presence/absence
df_cleaned[species_cols] = df_cleaned[species_cols].map(lambda x: 1 if x > 0 else 0)

# Step 7.5: Remove rows with no species present at all
df_cleaned = df_cleaned[df_cleaned[species_cols].sum(axis=1) > 0]

# Step 7.6: Optional: Handle missing biome
df_cleaned["BIOME"] = df_cleaned["BIOME"].fillna("Unknown")

# Step 7.7: Export cleaned dataset
df_cleaned.to_csv("species_biome_env_dataset.csv", index=False)
print("Dataset exported! Final shape:", df_cleaned.shape)